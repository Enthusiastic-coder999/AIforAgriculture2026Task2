{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNnCL-syTBle",
        "outputId": "18c89c15-6d4f-49e6-a2d1-f97f4fd2f6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/beyond-visible-spectrum-ai-for-agriculture-2026p2.zip\n",
            "replace ICPR02/kaggle/Aphid/0041231a3f6f4fa9b07a04234cef4627/B1.tif? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip /content/drive/MyDrive/beyond-visible-spectrum-ai-for-agriculture-2026p2.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tifffile as tiff\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "33ZMP9q0TbWt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import tifffile as tiff\n",
        "import numpy as np\n",
        "labeled_samples = glob.glob('/content/ICPR02/kaggle/**/B2.tif', recursive=True)\n",
        "unlabeled_samples = glob.glob('/content/archive/share/train/**/B2.tif', recursive=True)\n",
        "def analyze_sample(file_path, title=\"Sample\"):\n",
        "    folder = os.path.dirname(file_path)\n",
        "    print(f\"--- {title} Analysis ---\")\n",
        "    print(f\"Folder: {folder}\")\n",
        "    for b_name in ['B2', 'B12']:\n",
        "        b_path = os.path.join(folder, f\"{b_name}.tif\")\n",
        "        if os.path.exists(b_path):\n",
        "            data = tiff.imread(b_path)\n",
        "            print(f\"  {b_name}: Shape={data.shape}, Dtype={data.dtype}, Range=[{data.min()}, {data.max()}]\")\n",
        "        else:\n",
        "            print(f\"  {b_name}: NOT FOUND in this folder.\")\n",
        "if labeled_samples:\n",
        "    analyze_sample(labeled_samples[0], \"Labeled\")\n",
        "else:\n",
        "    print(\"No labeled B2.tif found. Check path: /content/ICPR02/kaggle\")\n",
        "print(\"\\n\")\n",
        "if unlabeled_samples:\n",
        "    analyze_sample(unlabeled_samples[0], \"Unlabeled\")\n",
        "else:\n",
        "    print(\"No unlabeled B2.tif found. Check path: /content/archive/share/train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mztjNaOjTide",
        "outputId": "e96d4b04-13b8-4e2d-d89f-c01f852a33a8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Labeled Analysis ---\n",
            "Folder: /content/ICPR02/kaggle/Rust/e3d938b17b2f491384c2fcaa52a40c53\n",
            "  B2: Shape=(264, 264), Dtype=uint16, Range=[1, 2910]\n",
            "  B12: Shape=(132, 132), Dtype=uint16, Range=[331, 4412]\n",
            "\n",
            "\n",
            "--- Unlabeled Analysis ---\n",
            "Folder: /content/archive/share/train/wheat/000225/20200420T100019_20200420T100021_T33UYU\n",
            "  B2: Shape=(264, 264), Dtype=uint16, Range=[116, 7164]\n",
            "  B12: Shape=(132, 132), Dtype=uint16, Range=[85, 7030]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import tifffile as tiff\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "mp.set_sharing_strategy('file_system')\n",
        "class SentinelSSLDataset(Dataset):\n",
        "    def __init__(self, root_dirs, target_size=(224, 224)):\n",
        "        self.target_size = target_size\n",
        "        self.folder_paths = []\n",
        "        self.bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
        "        for root in root_dirs:\n",
        "            search_pattern = os.path.join(root, \"**/B2.tif\")\n",
        "            found_files = glob.glob(search_pattern, recursive=True)\n",
        "            self.folder_paths.extend([os.path.dirname(f) for f in found_files])\n",
        "        print(f\"Dataset initialized with {len(self.folder_paths)} total samples.\")\n",
        "    def __len__(self):\n",
        "        return len(self.folder_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.folder_paths[idx]\n",
        "        band_list = []\n",
        "        for b in self.bands:\n",
        "            b_path = os.path.join(folder, f\"{b}.tif\")\n",
        "            if not os.path.exists(b_path):\n",
        "                alt_b = f\"B0{b[1:]}\" if len(b) == 2 else b\n",
        "                b_path = os.path.join(folder, f\"{alt_b}.tif\")\n",
        "            img = tiff.imread(b_path).astype(np.float32) / 10000.0\n",
        "            img_tensor = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)\n",
        "            img_resized = F.interpolate(img_tensor, size=self.target_size, mode='bilinear', align_corners=False)\n",
        "            band_list.append(img_resized.squeeze())\n",
        "        full_stack = torch.stack(band_list, dim=0)\n",
        "        return full_stack\n",
        "roots = ['/content/ICPR02/kaggle', '/content/archive/share/train']\n",
        "ssl_dataset = SentinelSSLDataset(roots)\n",
        "ssl_loader = DataLoader(ssl_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "for batch in ssl_loader:\n",
        "    print(f\"Batch shape: {batch.shape}\")\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Oks9U4cTvFz",
        "outputId": "a5ca6961-d332-40ab-c481-eda8c52c9edd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset initialized with 3468 total samples.\n",
            "Batch shape: torch.Size([16, 12, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LabeledSentinelDataset(Dataset):\n",
        "    def __init__(self, root_dir, target_size=(224, 224)):\n",
        "        self.target_size = target_size\n",
        "        self.bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']\n",
        "\n",
        "        self.folder_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        self.class_to_idx = {'blast': 0, 'brown': 1, 'healthy': 2, 'rust': 3}\n",
        "\n",
        "        search_pattern = os.path.join(root_dir, \"**/B2.tif\")\n",
        "        found_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "        for f in found_files:\n",
        "            folder = os.path.dirname(f)\n",
        "            class_name = folder.split('/')[-2].lower()\n",
        "            if class_name in self.class_to_idx:\n",
        "                self.folder_paths.append(folder)\n",
        "                self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.folder_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder = self.folder_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        band_list = []\n",
        "\n",
        "        for b in self.bands:\n",
        "            b_path = os.path.join(folder, f\"{b}.tif\")\n",
        "            if not os.path.exists(b_path):\n",
        "                alt_b = f\"B0{b[1:]}\" if len(b) == 2 else b\n",
        "                b_path = os.path.join(folder, f\"{alt_b}.tif\")\n",
        "\n",
        "            img = tiff.imread(b_path).astype(np.float32) / 10000.0\n",
        "            img_tensor = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)\n",
        "            img_resized = F.interpolate(img_tensor, size=self.target_size, mode='bilinear', align_corners=False)\n",
        "            band_list.append(img_resized.squeeze())\n",
        "\n",
        "        return torch.stack(band_list, dim=0), label"
      ],
      "metadata": {
        "id": "alADT7FpT6HZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_path = '/content/ICPR02/kaggle'\n",
        "labeled_dataset = LabeledSentinelDataset(root_dir=root_path)\n",
        "labeled_loader = DataLoader(\n",
        "    labeled_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Successfully created loader with {len(labeled_dataset)} samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6g5wmgmWthY",
        "outputId": "7a7771ef-86ec-4e76-b8d6-3a41962d5ac5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created loader with 115 samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.8 * len(labeled_dataset))\n",
        "val_size = len(labeled_dataset) - train_size\n",
        "train_ds, val_ds = random_split(labeled_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "HYcCQFh5Xw4a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class BaselineCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4, input_channels=12):\n",
        "        super(BaselineCNN, self).__init__()\n",
        "        self.network = models.resnet18(weights=None)\n",
        "\n",
        "        self.network.conv1 = nn.Conv2d(\n",
        "            input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
        "        )\n",
        "\n",
        "        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "wJoGvhSSV_ol"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BaselineCNN(num_classes=4).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            val_correct += (pred == labels).sum().item()\n",
        "\n",
        "    val_acc = 100 * val_correct / val_size\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGkvWVCYUAAW",
        "outputId": "5ba79706-0c4c-4fac-84a0-2760639f8683"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.1626 | Val Acc: 60.87%\n",
            "Epoch 2: Train Loss: 0.8039 | Val Acc: 60.87%\n",
            "Epoch 3: Train Loss: 0.4016 | Val Acc: 39.13%\n",
            "Epoch 4: Train Loss: 0.3360 | Val Acc: 69.57%\n",
            "Epoch 5: Train Loss: 0.1738 | Val Acc: 73.91%\n"
          ]
        }
      ]
    }
  ]
}